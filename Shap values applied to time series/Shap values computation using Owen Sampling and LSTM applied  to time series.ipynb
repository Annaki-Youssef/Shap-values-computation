{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import datasets, model_selection, preprocessing, model_selection\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "%matplotlib inline \n",
    "\n",
    "#####################\n",
    "# Time Series loading\n",
    "#####################\n",
    "df1 = pd.read_csv('/content/AIR.FR-d-20200701.csv', sep=' ', names=['name', 'date', 'time', 'val', 'extra'])\n",
    "df2 = pd.read_csv('/content/BNP.FR-d-20200701.csv', sep=' ', names=['name', 'date', 'time', 'val', 'extra'])\n",
    "df3 = pd.read_csv('/content/FP.FR-d-20200701.csv', sep=' ', names=['name', 'date', 'time', 'val', 'extra'])\n",
    "df4 = pd.read_csv('/content/DG.FR-d-20200701.csv', sep=' ', names=['name', 'date', 'time', 'val', 'extra'])\n",
    "df5 = pd.read_csv('/content/MC.FR-d-20200701.csv', sep=' ', names=['name', 'date', 'time', 'val', 'extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Data pre-processing\n",
    "#####################\n",
    "for c in [df1, df2, df3, df4, df5]:\n",
    "  c.drop('extra', axis=1, inplace=True)\n",
    "\n",
    "  c['datetime'] = c[['date', 'time']].agg(' '.join, axis=1)\n",
    "  c['datetime'] = c['datetime'].astype('datetime64') #'datetime64[ns]'\n",
    "\n",
    "  c.drop(['date', 'time'], axis=1, inplace=True)\n",
    "\n",
    "# Time series classes\n",
    "\n",
    "class TSElement(object):\n",
    "    def __init__(self,dt, val=0):\n",
    "        self.dt = dt\n",
    "        self.val = val\n",
    "        \n",
    "    def __str__(self):\n",
    "        string = []\n",
    "        \n",
    "        id_dt= self.dt\n",
    "        string.append(f'Datetime: {self.dt}')\n",
    "        \n",
    "        id_val = self.val\n",
    "        string.append(f'Value : {self.val}')\n",
    "        \n",
    "        return string\n",
    "        \n",
    "class TimeSeriesData:\n",
    "    def __init__(self, df, name, h=3):\n",
    "        self.name = name\n",
    "        self.h = h\n",
    "        self.instances = None\n",
    "        #self.bs = bs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(df) - self.h\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        l = list(zip(df['datetime'],df['val']))\n",
    "        self.instances  = [[TSElement(*o) for o in l[i:i+self.h]] for i,e in enumerate(l) if (i+self.h) < len(df)]\n",
    "        print('Size of Dataset:', len(self.instances))\n",
    "\n",
    "df1 = df1.set_index('datetime')\n",
    "df2 = df2.set_index('datetime')\n",
    "df3 = df3.set_index('datetime')\n",
    "df4 = df4.set_index('datetime')\n",
    "df5 = df5.set_index('datetime')\n",
    "##\n",
    "h = 1000 # We will focus on the first h values of the time series (~250 minutes) to reduce the computation time\n",
    "df1, df2, df3, df4, df5 = df1[:h], df2[:h], df3[:h], df4[:h], df5[:h]\n",
    "\n",
    "df = df1.copy() \n",
    "df['val1'], df['val2'], df['val3'], df['val4'], df['val5'] = df['val'], df2['val'], df3['val'], d4['val'], df5['val']\n",
    "df.drop(['val'], axis=1, inplace = True)\n",
    "df.drop(['name'], axis=1, inplace = True)\n",
    "\n",
    "# From time series to supervised learning\n",
    "def split_series(series, n_past, n_future):\n",
    "  #\n",
    "  # n_past ==> no of past observations used for prediction\n",
    "  #\n",
    "  # n_future ==> no of future observations (predictions) \n",
    "  #\n",
    "  X, y = list(), list()\n",
    "  for window_start in range(len(series)):\n",
    "    past_end = window_start + n_past\n",
    "    future_end = past_end + n_future\n",
    "    if future_end > len(series):\n",
    "      break\n",
    "    # slicing the past and future parts of the window\n",
    "    past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "    X.append(past)\n",
    "    y.append(future)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Causal inference between time series using \n",
    "#Owen sampling+Encoder-Decoder multi step LSTM Model With Multivariate Input to compute Shapley values\n",
    "######################################################################################################\n",
    "\n",
    "w = 50\n",
    "Q = 50                     # Integral's discretisation parameter (Rectangle Rule: for simplicity). Other Rules can be used !\n",
    "M = 50                     # Sample Size for the empirical estimator of the expectation\n",
    "df2 = df.copy()\n",
    "d = len(df2.columns)\n",
    "D = {}                     # Dictionary saving the different causal relations. In fact, each \"key\" will represente a time serie and D[\"key\"] contains the shapley values related to the causal realtionships going from each time serie to \"key\" .\"                 \n",
    "for k1 in range(d):\n",
    "    D['val'+str(k1)] = []\n",
    "    Sh = np.zeros([1,d])[0]\n",
    "    for k in range(0,Q+1):\n",
    "        e = np.zeros([1,d])[0]\n",
    "        for m in range(M):\n",
    "            B = bernoulli.rvs(k/Q, size = d)\n",
    "            while list(B) != list(np.ones([1,d])[0]):\n",
    "                I = B\n",
    "                break\n",
    "            for j in range(d):\n",
    "                X_j = np.zeros([1,d])[0]\n",
    "                X_j[j] = 1\n",
    "\n",
    "                L2, K = [], []\n",
    "                for i in range(d):\n",
    "                    L2.append(int(I[i])*df2.columns[i])\n",
    "                    K.append((int(X_j[i])*df2.columns[i]))\n",
    "                L1 = list(set(L2 + K))\n",
    "                L2 = list(set((L2)))\n",
    "                L1.remove('')\n",
    "                L2.remove('')\n",
    "\n",
    "                ###############\n",
    "                # LSTM forecast\n",
    "                ###############\n",
    "\n",
    "                data1 = df2[L1]\n",
    "                r = int(0.97*len(data1))\n",
    "                train_df,test_df = data1[:r], data1[r:] \n",
    "\n",
    "                # Scaling data\n",
    "                train = train_df\n",
    "                scalers={}\n",
    "                for i in train_df.columns:\n",
    "                    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                    s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
    "                    s_s = np.reshape(s_s,len(s_s))\n",
    "                    scalers['scaler_'+ i] = scaler\n",
    "                    train.replace(np.array(train[i]), s_s)\n",
    "                test = test_df\n",
    "                for i in train_df.columns:\n",
    "                    scaler = scalers['scaler_'+i]\n",
    "                    s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
    "                    s_s = np.reshape(s_s,len(s_s))\n",
    "                    scalers['scaler_'+i] = scaler\n",
    "                    test.replace(np.array(test[i]), s_s)\n",
    "                #From time serie to supervised\n",
    "                n_past = 10\n",
    "                n_future = 5 \n",
    "                n_features = len(L1)\n",
    "\n",
    "                X_train, y_train = split_series(train.values,n_past, n_future)\n",
    "                X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\n",
    "                y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
    "                X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "                X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "                y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))\n",
    "\n",
    "                # E2D2\n",
    "                # n_features ==> no of features at each timestep in the data.\n",
    "                #\n",
    "                encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "                # encoder_l1 = tf.keras.layers.LSTM(100,return_sequences = True, return_state=True)\n",
    "                encoder_l1 = tf.compat.v1.keras.layers.CuDNNLSTM(100,return_sequences = True, return_state=True)\n",
    "                encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "                encoder_states1 = encoder_outputs1[1:]\n",
    "                # encoder_l2 = tf.keras.layers.LSTM(100, return_state=True)\n",
    "                encoder_l2 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_state=True)\n",
    "                encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "                encoder_states2 = encoder_outputs2[1:]\n",
    "                #\n",
    "                decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs2[0])\n",
    "                #\n",
    "                # decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "                decoder_l1 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "                # decoder_l2 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)\n",
    "                decoder_l2 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)\n",
    "                decoder_outputs2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l2)\n",
    "                #\n",
    "                model_e2d2 = tf.keras.models.Model(encoder_inputs,decoder_outputs2)\n",
    "                #\n",
    "                # model_e2d2.summary()\n",
    "\n",
    "                #Training the model\n",
    "                reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "                model_e2d2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "                history_e2d2=model_e2d2.fit(X_train,y_train,epochs=25,validation_data=(X_test,y_test),batch_size=32,verbose=0,callbacks=[reduce_lr])\n",
    "\n",
    "                #Predictions\n",
    "                pred_e2d2=model_e2d2.predict(X_test)\n",
    "\n",
    "                #Inverse scaling\n",
    "                for index,i in enumerate(train_df.columns):\n",
    "                    scaler = scalers['scaler_'+i]\n",
    "                    pred_e2d2[:,:,index]=scaler.inverse_transform(pred_e2d2[:,:,index])\n",
    "                    y_train[:,:,index]=scaler.inverse_transform(y_train[:,:,index])\n",
    "                    y_test[:,:,index]=scaler.inverse_transform(y_test[:,:,index])\n",
    "\n",
    "                #Checking Errors\n",
    "                b = 0\n",
    "                for j1 in range(1,6):\n",
    "                    b += mean_absolute_error(y_test[:,j1-1,k1],pred_e2d2[:,j1-1,k1])\n",
    "                b = b/n_features\n",
    "\n",
    "                c1 = 1-b\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "                if L2 == []:\n",
    "                    c2 = 0\n",
    "                else:\n",
    "\n",
    "                    data2 = df2[L2]\n",
    "                    r = int(0.97*len(data2))\n",
    "                    train_df,test_df = data2[:r], data2[r:] \n",
    "\n",
    "                    # Scaling data\n",
    "                    train = train_df\n",
    "                    scalers={}\n",
    "                    for i in train_df.columns:\n",
    "                        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                        s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
    "                        s_s = np.reshape(s_s,len(s_s))\n",
    "                        scalers['scaler_'+ i] = scaler\n",
    "                        train.replace(np.array(train[i]), s_s)\n",
    "                    test = test_df\n",
    "                    for i in train_df.columns:\n",
    "                        scaler = scalers['scaler_'+i]\n",
    "                        s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
    "                        s_s = np.reshape(s_s,len(s_s))\n",
    "                        scalers['scaler_'+i] = scaler\n",
    "                        test.replace(np.array(test[i]), s_s)\n",
    "                    #From time serie to supervised\n",
    "                    n_past = 10\n",
    "                    n_future = 5 \n",
    "                    n_features = len(L2)\n",
    "\n",
    "                    X_train, y_train = split_series(train.values,n_past, n_future)\n",
    "                    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\n",
    "                    y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
    "                    X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "                    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "                    y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))\n",
    "\n",
    "                    # E2D2\n",
    "                    # n_features ==> no of features at each timestep in the data.\n",
    "                    #\n",
    "                    encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "                    # encoder_l1 = tf.keras.layers.LSTM(100,return_sequences = True, return_state=True)\n",
    "                    encoder_l1 = tf.compat.v1.keras.layers.CuDNNLSTM(100,return_sequences = True, return_state=True)\n",
    "                    encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "                    encoder_states1 = encoder_outputs1[1:]\n",
    "                    # encoder_l2 = tf.keras.layers.LSTM(100, return_state=True)\n",
    "                    encoder_l2 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_state=True)\n",
    "                    encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "                    encoder_states2 = encoder_outputs2[1:]\n",
    "                    #\n",
    "                    decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs2[0])\n",
    "                    #\n",
    "                    # decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "                    decoder_l1 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "                    # decoder_l2 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)\n",
    "                    decoder_l2 = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)\n",
    "                    decoder_outputs2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l2)\n",
    "                    #\n",
    "                    model_e2d2 = tf.keras.models.Model(encoder_inputs,decoder_outputs2)\n",
    "                    #\n",
    "                    # model_e2d2.summary()\n",
    "\n",
    "\n",
    "                    #Training the model \n",
    "                    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "                    model_e2d2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "                    history_e2d2=model_e2d2.fit(X_train,y_train,epochs=25,validation_data=(X_test,y_test),batch_size=32,verbose=0,callbacks=[reduce_lr])\n",
    "\n",
    "                    #Predictions\n",
    "                    pred_e2d2=model_e2d2.predict(X_test)\n",
    "\n",
    "                    #Inverse scaling\n",
    "                    for index,i in enumerate(train_df.columns):\n",
    "                        scaler = scalers['scaler_'+i]\n",
    "                        pred_e2d2[:,:,index]=scaler.inverse_transform(pred_e2d2[:,:,index])\n",
    "                        y_train[:,:,index]=scaler.inverse_transform(y_train[:,:,index])\n",
    "                        y_test[:,:,index]=scaler.inverse_transform(y_test[:,:,index])\n",
    "\n",
    "                    #Checking Errors\n",
    "                    b = 0\n",
    "                    for j1 in range(1,6):\n",
    "                      b += mean_absolute_error(y_test[:,j1-1,k1],pred_e2d2[:,j1-1,k1])\n",
    "                    b = b/n_features\n",
    "\n",
    "\n",
    "                    c2 = 1-b\n",
    "                          ##\n",
    "                    e[j] += (c1-c2)\n",
    "        Sh += (e/M)                   \n",
    "      \n",
    "    print(k1)\n",
    "    Sh = (Sh*(1/Q))  # Vector of Features' Shapley Values\n",
    "    D['val'+str(k1)].append(Sh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
